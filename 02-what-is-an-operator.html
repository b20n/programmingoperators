<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>What is an Operator?</title>
<meta name="generator" content="Org mode">
<link rel="stylesheet" type="text/css" href="style.css" />
</head>
<body>
<div id="preamble" class="status">
<ul><li><a href="01-introduction.html">Introduction</a></li></ul>
</div>
<div id="content">
<h1 class="title">What is an Operator?</h1>
<p>
There has been an explosion in the world's collective dependence on
software in the twenty-first century. "Software is eating the world",
more and more. Our cars, our thermostats, our power grids, our
communications, our businesses, our shopping - in many parts of the
world it's difficult to find an aspect of modern life that doesn't
depend deeply on software. "The Cloud" has compounded this phenomenon.
Even a task as mundane and nonsocial as doing your taxes is typically
done in a client-server model via a web browser instead of a
standalone application, let alone visiting a brick-and-mortar office
and talking to another human.
</p>

<p>
These factors in turn put a greater burden on the shoulders of the
developers and maintainers of this software. Modern software must be
highly scalable and highly available. Economic concerns demand that
this be done as cheaply as possible. Programmers are push to do more
with less.
</p>

<p>
Operators are a technology designed to reduce this burden by making it
easier to automate management of systems - particularly those that
resist automation by traditional means. They fill a gap in the average
programmer's toolkit for managing distributed stateful systems at
scale.
</p>

<p>
You might wonder what makes operators special, why they're
different from "traditional means", what that gap is, and how
they fill it. To answer these questions, let's take a brief detour
through the history of computer systems management and look at how we
got here and what we may have left behind.
</p>

<div id="outline-container-org2a2f27d" class="outline-2">
<h2 id="org2a2f27d"><span class="section-number-2">1</span> A brief history of managing computer systems</h2>
<div class="outline-text-2" id="text-1">
<p>
N.B.: what follows is a rational reconstruction; it is not intended to
be a comprehensive history by any means. There are loads of great
books out there on the history of computing and computers; this isn't
one of them.
</p>
</div>

<div id="outline-container-orgf93a2ee" class="outline-3">
<h3 id="orgf93a2ee"><span class="section-number-3">1.1</span> Beginnings: early electromechanical machines</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The earliest days of modern computing are almost unrecognizable - no
stored programs, no programming languages, no displays, machines that
take up entire rooms - but the computers of the 1940s such as the
Harvard Mark I and the ENIAC are unmistakably computers in the modern
sense. The fact that they are so different from the world of the
twenty-first century provides a useful starting point for our
investigation. From a systems management perspective, several points
are worth noting:
</p>

<ul class="org-ul">
<li><span class="underline">Precision is important, too:</span> In the context of the 1940s, one of
the key features of these early machines was their calculation
precision. The analog computers they replaced were precise only to
two or three significant digits. The Harvard Mark I could calculate
precisely to twenty-three digits. Unfortunately, unlike in modern
systems where failures typically manifest as availability loss,
component failure in an electromechanical computer could compromise
precision as well, resulting in silent corruption of results.</li>

<li><span class="underline">Component failures really matter</span>: These machines had good reason
to take up entire rooms. Without microchips, integrated circuits, or
even transistors, they were composed of millions of discrete
components, each subject to independent failure. There is an
interesting distinction between the Harvard Mark I and the later
ENIAC here. The Mark I engineers traded performance for reliability
by using multipole relays in place of vacuum tubes, resulting in a
machine that was able to operate nearly continuously for sixteen
years. ENIAC, on the other hand, relied on nearly 18,000 vacuum
tubes and rarely ran for more than a few days without blowing one
out. It was a far faster machine, but only achieved approximately
50% uptime.</li>

<li><span class="underline">Sysadmins, too</span>: These machines were extremely expensive,
specialized pieces of hardware, usually dedicated to military
purposes. Access to them was tightly controlled. ENIAC, for example,
was run by a very small team that developed both the programming
techniques as well as sophisticated operational procedures such as
"don't turn it off because it'll blow tubes when it boots up".</li>
</ul>

<p>
Finally, no discussion of systems management on electromechanical
copmuters would be complete without mentioning the Semi-Automatic
Ground Environment, or SAGE. SAGE was a Cold War-era missile defense
system designed to coordinate and aggregate radar data across North
America. It composed hundreds of radar systems feedling data to a set
of twenty four "Direction Centers". Every Direction Center housed a
pair of AN/FSQ-7 computers each of which individually was the largest
discrete computer ever built: it weighed 250 tons, required a 1
megawatt power supply, housed 49,000 vacuum tubes. These machines
operated 24x7 in an active/standby pair delivering 99.95% uptime. Data
from radars across the continent was fed to the Direction Centers on
phone lines at 1300 bps. From a management perspective SAGE was a
landmark. Not only was it tackling harder problems by introducing
network failures and their attendant distributed systems problems, it
also set a higher bar in attempting to overcome them.
</p>
</div>
</div>
</div>

<div id="outline-container-org6603e40" class="outline-2">
<h2 id="org6603e40"><span class="section-number-2">2</span> Transistors and commercialization</h2>
<div class="outline-text-2" id="text-2">
<p>
While some commercial machines did exist in the vacuum tube era the
mass productionization of transistors in the early 1960s marked both
the beginning of a boom in commercial computing and a revolution in
systems reliability. With solid-state transistors replacing vacuum
tubes at the core of the machines, reliability increased tremedously.
The attendant revoluion in operational cost along with the diminished
physical scale and capital requirements of the machines made computing
available to a much wider audience than ever before. This led roughly
down two paths. One, which we'll return to later on, starts with the
TX-0 at MIT and proceeds through to the DEC PDPs and the Internet. The
other, which we'll trace here, remains in the realm of "big iron"
computing - impersonal business machines built along the lines of
their first-generation predecessors but taking advantage of
transistors' scale and reliability to achieve results far surpassing
the electromechanical systems.
</p>

<p>
These machines - for example, the IBM 7000 series and the CDC 6000
series - were the first "mainframes"; some variants were the first to
be labeled "supercomputers". Applications were still largely local and
batch-oriented, not interactive. Administration of the systems was
still a specialized task, solidifying the "high priesthood of
computing" trope. Reliability concerns were still typically directed
toward single-system uptime via component testing and local
redundancy. Reliability across multiple systems was usually achieved by
running identical workloads on active/active pairs of machines at
great cost. This line of thinking - "I've got this one system running
my application, make it as fast and reliable as possible" may seem
quaint to today's programmers steeped in commodity hardware approaches
to reliability, but in some circles it's still preached as high art -
IBM does, after all, continue to produce new Z-series mainframes year
after year.
</p>
</div>
</div>



<div id="outline-container-org1c9a843" class="outline-2">
<h2 id="org1c9a843"><span class="section-number-2">3</span> Previous revision</h2>
<div class="outline-text-2" id="text-3">
<p>
The modern consumer expects Internet services that are always-on,
fast, and cheap. These expectations place a burden on the shoulders of
implementors and operators of these services, to deliver them in an
economical fashion. Over the past twenty years there has been an
explosion of tools, both technical and nontechnical, designed to meet
this challenge. DevOps, Site Reliability Engineering,
infrastructure-as-code, cattle-not-pets, containerization, NoSQL,
Kubernetes, serverless - all of these have played a part in the
evolution of the state of the art. To some extent the future is here
(Kubernetes would be near-unthinkable by the average engineer in 2000,
but is commonplace in 2020) but it's not evenly distributed.
</p>

<p>
Many of these tools focus to near-exclusivity on the problems of
stateless services, such as API servers and other so-called
"microservices". This makes some sense; stateless services are
relatively simple from an run-time point of view. But it's a rare
stateless service that exists without the backing of a stateful
system, such as a database or a message queue. And stateful systems
are another beast entirely.
</p>

<p>
Managing stateful systems effectively and economically - scaling,
upgrading, reconfiguring, backing up, recoving from failure - requires
careful application of domain knowledge that cannot be encoded in
Kubernetes. This isn't to say that it's not possible to deploy
stateful systems to Kubernetes, or that "operable" stateful systems
don't exist. But it is the case that as a community of practice we do
not have a shared understanding of how to do this in a repeatable,
reliable, and economical fashion. The closest things we have come from
the observability space, such as runbooks and alerts, and they don't
scale.
</p>

<p>
Operators are an answer to this problem. They are a tool for
delivering and managing stateful systems running on Kubernetes. An
operator is higher-level software that implements the behaviors to
manage a stateful system in a robust manner on top of the Kubernetes
runtime.
</p>

<p>
The heart of an operator is a controller. The controller periodically
inspects "the world" and compares to a representation of the desired
state of that world. If any action is necessary to make the world more
closely reflect the desired state, the controller takes that action.
This loop is performed in perpetuity. Think of a thermostat: every so
often, it observes the state of "the world" - in this case, the
ambient temperature - and compares it to the user's desired state of
that world - say, seventy degrees Farenheit. If the world is too cold,
the furnace is powered on. If the world is too hot, perhaps the
furnace will be powered off. Some time later, the thermostat will
inspect the state of the world again, compare to the user's desired
state (perhaps seventy-one degrees this time!) and again take
necessary action.
</p>

<p>
Despite the apparent triviality of the thermostat example it turns out
that this pattern can be effectively applied to solve a broad range of
systems management problems. This is not an accident; the
fundamentals of controllers have been well-explored and systematized
in the field of control theory which dates back to Maxwell in the
nineteenth century. There's a big difference between an operator that
provisions an object storage bucket and a PID controller that
auto-tunes a pool of web servers, but the basic idea is pretty
similar, once you've got a control theory lens on the world.
</p>

<p>
In the context of of Kubernetes, a minimal operator comprises two
components: a special type of resource called a
<code>CustomResourceDefinition</code>, or <code>CRD</code> and a controller that uses
instances of that <code>CRD</code> as the specification of the desired state of
the world.
</p>

<p>
An instance of a <code>CRD</code> is a Kubernetes resource like any other. It
includes standard <code>ObjectMeta</code> fields such as <code>Name</code>,
<code>CreationTimestamp</code>, <code>ResourceVersion</code>, and <code>Generation</code>, as well as
<code>Spec</code> and <code>Status</code> fields whose underlying structure is in the hands
of the implementor. For example, the manifest below defines a custom
resource called "<code>Foo</code> " in the <code>programmingoperators.com/v1</code> namespace:
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: foos.programmingoperators.com
spec:
  group: programmingoperators.com
  names:
    plural: foos
    singular: foo
    kind: Foo
  versions:
    - name: v1
      served: true
      storage: true
      subresources:
        # status enables the status subresource.
        status: {}
</pre>
</div>

<p>
And here's a sample manifest of an instance of that <code>CRD</code>:
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: "programmingoperators.com/v1"
kind: Foo
metadata:
  name: my-foo
spec:
  [ ... ]
status:
  [ ... ]
</pre>
</div>

<p>
A controller associated with this resource uses the Kubernetes APIs to
watch instances of <code>foos.programmingoperators.com/v1</code> and take action
against "the world" when necessary. It might create or mutate other
Kubernetes resources, reconfigure a database, or launch missiles;
beyond some basic limitations, Kubernetes doesn't much care.
</p>

<div class="org-src-container">
<pre class="src src-go"><span style="color: #00ff00;">func</span><span style="color: #000000; background-color: #7f7f7f;"> </span><span style="color: #0000ff;">Sync</span>(foo<span style="color: #000000; background-color: #7f7f7f;"> </span>*<span style="color: #ffff00;">v1.Foo</span>)<span style="color: #000000; background-color: #7f7f7f;"> </span>{
<span style="color: #cd00cd; background-color: #7f7f7f;">    </span>TODO
}
</pre>
</div>

<p>
That's it, really. "Operator" may imply big ideas about autonomous,
self-healing distributed systems and thousands of SREs on the streets
looking for jobs, but it doesn't entail them. An operator can be as
big or as small as the role it needs to fill with a system.
</p>

<p>
Most of the rest of this book is about how to turn this trivial
example in to something worthwhile and useful. There's a lot to cover.
To dip in just a bit deeper let's touch on two big ideas.
</p>

<p>
The first is related to the fact that Operators exist at the behest of
the Kubernetes API, and that API has very particular semantics. One of
these semantics, and perhaps the most important for operator
programmers, is that there is no way to observe every change to a
resource. Whatever your operator needs to do, it neds to do based on
the current state of the resource at any given time. This idea is
sometimes called "level-triggering", and it's essential to writing
operators that do what they say on the box. We'll be talking about it
quite a bit.
</p>

<p>
The second big idea is convergence. Given a resource with a certain
specification, the controller should eventually converge to a stable
state where no more action is taken. "Eventually" could be a very long
time - hundreds of iterations, or hours in the real-world - but the
controller should be working to align the state of the world with the
state specified in the resources it manages. If that specified state
changes - even before the world has been converged - the controller
should start moving towards the new state.
</p>
</div>
</div>
</div>
</body>
</html>
